{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0l9JlVCHxMC"
      },
      "source": [
        "# **Project overview**\n",
        "The Enigma machine is a famous encryption device used by the Germans, most notably during World War II, for securing military communications. The machine resembles a typewriter and operates by inputting letters through a keyboard, which are then encrypted based on the configuration of several rotors and a plugboard before being output as a different letter.\n",
        "\n",
        "This setup allowed for a vast number of possible settings, making the Enigma's encryption extremely difficult to break.  \n",
        "We will try to Break the enigma code using modern tools and not just in terms of computing power.  \n",
        "We will use spesificlly machine learning tools: AdaBoost, Random Forest, SVM and Neural Network (more precisely LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz3Bq6zRD-dN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "# Adjusted char_to_int function to handle a broader range of characters safely\n",
        "def char_to_int(char):\n",
        "    if 'a' <= char <= 'z':\n",
        "        return ord(char) - ord('a')\n",
        "    # Add more conditions here for other characters in your dataset\n",
        "    elif char == ' ':\n",
        "      print(\"char found!\")\n",
        "      return 26\n",
        "    # Handle unexpected characters\n",
        "    else:\n",
        "      print(\"unexpected characters found!\")\n",
        "      return 27\n",
        "\n",
        "# Function to define int to char (inverse of char_to_int)\n",
        "def int_to_char(i):\n",
        "    return chr(i + ord('a'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rELTzlm0KsHH"
      },
      "source": [
        "# **Data Preparation**\n",
        "The Data is a collection of words and their encoding.\n",
        "every word starts in the same state of the machine so the model will have to understand the relations between the Enigma parts in order to understand the current configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDgxhM7nKs0I"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Path to your dataset\n",
        "# file_path = '/content/sample_data/data_bi_direction.txt'\n",
        "file_path = '/content/sample_data/data.txt'\n",
        "\n",
        "# Read the dataset\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    data = [line.strip().split(' ') for line in lines]\n",
        "\n",
        "\n",
        "# Extract origin words and encoded words\n",
        "origin_words = [item[0] for item in data]\n",
        "encoded_words = [item[1] for item in data]\n",
        "\n",
        "# Encode words as sequences of integers\n",
        "encoded_origin_words = [[char_to_int(char) for char in word] for word in origin_words]\n",
        "encoded_encoded_words = [[char_to_int(char) for char in word] for word in encoded_words]\n",
        "\n",
        "max_word_length = max(max(len(word) for word in origin_words), max(len(word) for word in encoded_words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSTa-drqIM7F"
      },
      "source": [
        "# **AdaBoost - Using Decision Tree**\n",
        "First we will try to use AdaBoost - It's seems requested since the algorithm combines multiple weak learners to create a strong learner, so we can use each letter permution in a given level (letter index) as a weak rule - combine them and mabey the algorithem will mannage to learn the Enigma configuration pattern\n",
        "\n",
        "There is still one issue we can think about it - even in case this algorithm will work perfectly, It still always be blocked on the longest word in the Dataset - since that our rules are set only for the max size of the Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg0UTr_jLUK5"
      },
      "source": [
        "## **Step 1: Data Preprocessing**\n",
        "\n",
        "AdaBoost typically deals with fixed-length feature vectors, so we'll need to address the variable length of words -> our approach is to pad sequences to have the same length, ensuring that all input vectors are of equal length.\n",
        "\n",
        "Be aware that we are dealing with a classification problem where each position in a word could potentially be classified into one of 28 classes (26 letters + space + unexpected characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1lJW5JCLdPf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Function to pad encoded word sequences to the same length\n",
        "def pad_encoded_sequences(encoded_sequences, max_length):\n",
        "    return [seq + [27] * (max_length - len(seq)) for seq in encoded_sequences]  # 27 is used for padding\n",
        "\n",
        "# Pad the encoded sequences\n",
        "padded_encoded_origin_words = pad_encoded_sequences(encoded_origin_words, max_word_length)\n",
        "padded_encoded_encoded_words = pad_encoded_sequences(encoded_encoded_words, max_word_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj8XY06fLiQw"
      },
      "source": [
        "## **Step 2: Splitting the Data**\n",
        "Split our data into a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcixK-1_LnpW"
      },
      "outputs": [],
      "source": [
        "# Assuming padded_encoded_origin_words is your input data and padded_encoded_encoded_words is your target data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_encoded_origin_words, padded_encoded_encoded_words, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaSV806rLtGU"
      },
      "source": [
        "## **Step 3: Model Building and Training**\n",
        "Train an AdaBoost classifier for each position using the training data - For each position in the word, train a separate AdaBoost classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK3DzBzyL0CY"
      },
      "outputs": [],
      "source": [
        "classifiers = []  # store the classifiers for each position\n",
        "\n",
        "# timer - start\n",
        "adaboost_time_start = time.time()\n",
        "\n",
        "# max_word_length is the maximum word length\n",
        "for position in range(max_word_length):\n",
        "    # Prepare the dataset for the current position\n",
        "    X = np.array([word[position] for word in X_train if position < len(word)])\n",
        "    y = np.array([word[position] for word in y_train if position < len(word)])\n",
        "\n",
        "    # Initialize and train the AdaBoost classifier for the current position\n",
        "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50)\n",
        "    clf.fit(X.reshape(-1, 1), y)  # X needs to be reshaped because scikit-learn expects 2D arrays for X\n",
        "\n",
        "    classifiers.append(clf)\n",
        "    # Now, classifiers[i] is the AdaBoost model for the i-th position in the word.\n",
        "\n",
        "# timer - end\n",
        "adaboost_time_end = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avFMIB-9NJ3h"
      },
      "source": [
        "## **Step 4: Evaluation**\n",
        "For each position, predict the encoded character using the test set and calculate the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtyVw3BxNPrH",
        "outputId": "7692623c-b657-4809-b880-205626a3d5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.9295777646903634\n",
            "AdaBoost fit time: 393.9909701347351\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracies = []  # To store the accuracy of each classifier\n",
        "\n",
        "for position in range(max_word_length):\n",
        "    # Extract the test data for the current position\n",
        "    X_test_position = np.array([word[position] for word in X_test if position < len(word)])\n",
        "    y_test_position = np.array([word[position] for word in y_test if position < len(word)])\n",
        "\n",
        "    # Predict using the classifier for the current position\n",
        "    y_pred = classifiers[position].predict(X_test_position.reshape(-1, 1))\n",
        "\n",
        "    # Calculate the accuracy for the current position\n",
        "    accuracy = accuracy_score(y_test_position, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the overall accuracy\n",
        "overall_accuracy = np.mean(accuracies)\n",
        "print(f\"AdaBoost Accuracy: {overall_accuracy}\")\n",
        "print(f'AdaBoost fit time: {adaboost_time_end - adaboost_time_start}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression - Random Forest Regressor**\n",
        "Second we will try Regression. Since this problem is not involve predicting a continuous target variable - but predicting many variables together (sequence of letters encoding) we will need ***Multi-output Regression***, where the goal is to predict multiple target variables instead of just one.\n",
        "\n",
        "This method is a good pick since Multi-output Regression works best when variables are interrelated - and in our case the enigma changes it's configuration in each index (due to the rotors non stop movment) so in each letter you would exepect a different output.\n",
        "\n",
        "After some tests with Multi-output Regression model we found **Random Forest Regressor** is the best suit for our task - it's an ensemble learning technique that combines multiple decision trees to provide an accurate predictions for regression tasks."
      ],
      "metadata": {
        "id": "_s0yKgM0RwMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Data Preprocessing**\n",
        "Pad the Encoded Words to ensure they all have the same length.\n",
        "\n",
        "This step is necessary for creating a consistent input and output shape for our model.\n"
      ],
      "metadata": {
        "id": "cogTDJ47SHwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Function to pad the encoded words to the same length\n",
        "def pad_encoded_words(encoded_words, max_length):\n",
        "    return np.array([np.pad(word, (0, max_length - len(word)), 'constant', constant_values=27) for word in encoded_words])\n",
        "\n",
        "# Apply padding\n",
        "padded_encoded_origin_words = pad_encoded_words(encoded_origin_words, max_word_length)\n",
        "padded_encoded_encoded_words = pad_encoded_words(encoded_encoded_words, max_word_length)"
      ],
      "metadata": {
        "id": "6LORHIIOSHfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: One-Hot Encoding**\n",
        "This step converts the integer-encoded characters into a binary matrix representation suitable for regression. Each character (including the padding) is represented as a binary vector with all zeros except for the index of the character, which is set to 1.\n",
        "\n",
        "One-hot encoding is a process used to convert data into a format that can be provided to machine learning algorithms to do a better job in prediction. It works by converting each value into a new categorical column and assigns a 1 or 0 (True/False) value to those columns in each row."
      ],
      "metadata": {
        "id": "rC04N5fXWAXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding the data\n",
        "encoder = OneHotEncoder()\n",
        "encoded_inputs = encoder.fit_transform(padded_encoded_origin_words).toarray()\n",
        "encoded_outputs = encoder.fit_transform(padded_encoded_encoded_words).toarray()"
      ],
      "metadata": {
        "id": "mqXX0mK5WNLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Splitting the Data**\n",
        "Split the Dataset into training and testing sets to evaluate the performance of the model.\n"
      ],
      "metadata": {
        "id": "bz0OMCYoSlci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_encoded_origin_words, padded_encoded_encoded_words, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "YDRkB7psVU--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Model Definition and Training**\n",
        "We use sklearn.ensemble.RandomForestRegressor - A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
        "\n",
        "\n",
        "\n",
        "*   n_estimators - The number of trees in the forest.\n",
        "*   random_state - Controls both the randomness of the bootstrapping of the samples used when building trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "iF7Fsnm1WVLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "regression_time_start = time.time()\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "regression_time_end = time.time()"
      ],
      "metadata": {
        "id": "xE5lTbmLW-w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Evaluation**\n",
        " calculate accuracy"
      ],
      "metadata": {
        "id": "SFqm8EJXXIrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "# Here, we use mean squared error as a simple evaluation metric, but you can choose metrics that best suit your needs.\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Regression Mean Squared Error: {mse}')\n",
        "\n",
        "def rounded_accuracy(y_true, y_pred):\n",
        "    # Round predictions to nearest integer\n",
        "    rounded_preds = np.round(y_pred)\n",
        "    # Calculate accuracy\n",
        "    correct_predictions = np.sum(rounded_preds == y_true, axis=1)\n",
        "    total_predictions = y_true.shape[1]\n",
        "    # Average accuracy per word\n",
        "    accuracy_per_word = correct_predictions / total_predictions\n",
        "    # Overall accuracy\n",
        "    overall_accuracy = np.mean(accuracy_per_word)\n",
        "    return overall_accuracy\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = rounded_accuracy(y_test, y_pred)\n",
        "print(f\"Regression Accuracy: {accuracy}\")\n",
        "print(f'Regression Fit time: {regression_time_end - regression_time_start}')"
      ],
      "metadata": {
        "id": "M7CxHTTqXLtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e6b8c6-bcdd-4b87-adaa-a12fe1860b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Mean Squared Error: 0.38119596401650485\n",
            "Regression Accuracy: 0.9859030889496835\n",
            "Regression Fit time: 145.946448802948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8qoaf5MlsSf"
      },
      "source": [
        "# **SVM**\n",
        "Now, we experimenting indirectly use SVM for such a task (break the enigma), one unconventional approach could involve treating it as a *classification problem*, where each original letter maps to an encoded letter. This requires a significantly different setup.\n",
        "\n",
        "First we tried an word to word encoding  - that was a problematic classifiction problem since In a typical classification or regression task, y is expected to be a 1D array, where each element corresponds to a single label or value per input sample. However, in our case, each input sample (a word) is associated with multiple labels (the encoded characters), making it a multilabel classification problem.\n",
        "\n",
        "So We need a different approch...\n",
        "\n",
        "We will try to Simplify the Problem - focusing on character-to-character encoding rather than whole words. This approach significantly reduces complexity.\n",
        "This approach treats each character in the dataset as a separate data point, where the input feature is the original character, and the target output is the encoded character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phYRmi79A8mR"
      },
      "source": [
        "## **Step 1: Data Preprocessing**\n",
        "\n",
        "\n",
        "*   Flatten the dataset so that each character and its encoded counterpart form a data point.\n",
        "*   Encode characters as integer values (Given the nature of SVM  we'll stick with integer encoding).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC_-c1sSBBQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80b81f6-bed2-4ad6-9497-7aba5e125c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(encoded_origin_words):  418586\n",
            "split_point:  52323\n",
            "SVM data set size:  279274\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# since the data is pairs of chars - split the data for faster results\n",
        "split_point = len(encoded_origin_words) // 8\n",
        "print('len(encoded_origin_words): ', len(encoded_origin_words))\n",
        "print('split_point: ', split_point)\n",
        "\n",
        "# Use only the first half of each list\n",
        "splited_encoded_origin_words = encoded_origin_words[:split_point]\n",
        "splited_encoded_encoded_words = encoded_encoded_words[:split_point]\n",
        "\n",
        "data_size = 0\n",
        "\n",
        "for word in splited_encoded_origin_words:\n",
        "  data_size += len(word)\n",
        "\n",
        "print('SVM data set size: ', data_size)\n",
        "\n",
        "# Flatten the dataset to character level\n",
        "X = []\n",
        "y = []\n",
        "for original, encoded in zip(splited_encoded_origin_words, splited_encoded_encoded_words):\n",
        "    X.extend(original)\n",
        "    y.extend(encoded)\n",
        "\n",
        "# Ensure X and y are numpy arrays\n",
        "X = np.array(X).reshape(-1, 1)  # Reshape for sklearn compatibility\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzmpdatFB-JS"
      },
      "source": [
        "## **Step 2: Splitting the Data**\n",
        "Split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g_hbXdJCM9d"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biUb0-McCpJX"
      },
      "source": [
        "## **Step 3: Model Building and Training**\n",
        "Train an SVM model on the character-to-character mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c1CcHsJCuHG"
      },
      "outputs": [],
      "source": [
        "svm_time_start = time.time()\n",
        "\n",
        "# Train the SVM model\n",
        "model = SVC(kernel='rbf', gamma='scale')  # 'scale' is usually a good default for gamma\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "svm_time_end = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paLOVra9C2Un"
      },
      "source": [
        "## **Step 4: Evaluation**\n",
        "Evaluate the model's performance on the test set to see how well it predicts the encoding for unseen characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35KrR7vYDJqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236d3878-1b16-4fc4-a639-1134c029576d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.298379733237848\n",
            "SVM fit time: -1278.2595884799957\n"
          ]
        }
      ],
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'SVM Accuracy: {accuracy}')\n",
        "print(f'SVM fit time: {svm_time_end - svm_time_start}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgWJZz-MDAXM"
      },
      "source": [
        "# **LSTM**\n",
        "Lastly, After consulting with Liad that sugessted to look into RNN's models - we found the stateful LSTM model: To capture the evolving state of the Enigma machine, the stateful LSTM model seemed like the right choise.\n",
        "\n",
        "Stateful LSTMs have the ability to maintain state across batches, meaning that the state of the machine after encrypting one word can be carried over to the next word, mimicking the continuous operation of the Enigma machine.\n",
        "\n",
        "But This requireed careful batch preparation, ensuring that the sequence of words is maintained and that batches start where the previous batch left off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqumqnWfaxoc"
      },
      "source": [
        "## **Step 1: Data Preprocessing**\n",
        "\n",
        "We need to encode the letters to numerical values and prepare the data for sequence modeling.\n",
        "\n",
        "* Character Encoding: Map each unique character to an integer. Since the Enigma machine operates on letters, this will likely be a mapping of the alphabet.\n",
        "* Sequence Creation: Create sequences of these integers. Given the Enigma machine's stateful nature, each input letter's encoding must be followed by its corresponding output letter's encoding.\n",
        "* Data Structuring: For a stateful LSTM, it's crucial that the data fed into the model allows for the state to carry over from one batch to the next appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTNoGM1DcR2S"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.models import Sequential  # Import Sequential model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed  # Ensure all layers are imported\n",
        "\n",
        "\n",
        "# Pad sequences to the maximum word length to ensure uniform input size\n",
        "X = pad_sequences(encoded_origin_words , maxlen=max_word_length, padding='post')\n",
        "y = pad_sequences(encoded_encoded_words, maxlen=max_word_length, padding='post')\n",
        "\n",
        "# Assuming the dataset might contain lowercase letters and potentially other characters.\n",
        "chars = sorted(list(set(''.join(origin_words) + ''.join(encoded_words))))\n",
        "num_classes = len(chars)  # This should include all unique characters\n",
        "\n",
        "# Update y to categorical\n",
        "y = np.array([to_categorical(encoded_word, num_classes=len(chars)) for encoded_word in y])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7udnB0xcWMl"
      },
      "source": [
        "## **Step 2: Splitting the Data**\n",
        "\n",
        "Split the dataset into training, validation, and test sets using scikit-learn's train_test_split function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtzwMfhqYXd8"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the remaining data into validation and test sets (50% validation, 50% test)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUEsR5nmcgJY"
      },
      "source": [
        "## **Step 3: Model Building and Training**\n",
        "\n",
        "\n",
        "Modify the model to fit it with the training data and validate using the validation data. Ensure to reset states appropriately.\n",
        "\n",
        "We'll construct a simple stateful LSTM model using a deep learning framework Keras.\n",
        "\n",
        "The model have:\n",
        "\n",
        "1. An Embedding layer to convert integer representations of letters into dense vector embeddings.\n",
        "2. A Stateful LSTM layer to capture the sequences' dependencies, considering the Enigma's rotor movement.\n",
        "3. A Dense layer with a softmax activation to map the LSTM's output to a probability distribution over possible output letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbdaECf0ax93",
        "outputId": "ce4dc3e1-64da-4342-97e4-04198daf279e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "5233/5233 [==============================] - 116s 22ms/step - loss: 0.2106 - accuracy: 0.9463 - val_loss: 0.0306 - val_accuracy: 0.9925\n",
            "Epoch 2/2\n",
            "5233/5233 [==============================] - 114s 22ms/step - loss: 0.0338 - accuracy: 0.9910 - val_loss: 0.0110 - val_accuracy: 0.9974\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Embedding(len(chars), 10, input_length=max_word_length),  # Adjust input_length to max_word_length\n",
        "    LSTM(50, return_sequences=True),  # return_sequences=True for sequence prediction\n",
        "    TimeDistributed(Dense(len(chars), activation='softmax'))  # Predict a sequence\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Adjust the training process to no longer require reshaping of X and resetting states after each batch\n",
        "\n",
        "lstm_time_start = time.time()\n",
        "\n",
        "# Example training call, adjust epochs and batch_size as needed\n",
        "model.fit(X, y, epochs=2, batch_size=64, validation_split=0.2)\n",
        "\n",
        "lstm_time_end = time.time()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDk31U76cjNV"
      },
      "source": [
        "## **Step 4: Evaluation**\n",
        "Finally, evaluate the model's performance on the test dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ufhk0EGcXtv",
        "outputId": "7a3192b5-7e81-4007-d6ed-ee5df30a9b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41859/41859 [==============================] - 102s 2ms/step - loss: 0.0238 - accuracy: 0.9936\n",
            "LSTM Test Loss: 0.02377822995185852\n",
            "LSTM Accuracy: 0.993604302406311\n",
            "LSTM fit time: 236.70498871803284\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, batch_size=1)\n",
        "model.reset_states()  # Reset the states for the next evaluation or prediction\n",
        "print(f'LSTM Test Loss: {loss}')\n",
        "print(f'LSTM Accuracy: {accuracy}')\n",
        "print(f'LSTM fit time: {lstm_time_end - lstm_time_start}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czd47ANosnzm"
      },
      "source": [
        "## **Step 5: Lets See The Model In Action**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NEvKtiBswaK",
        "outputId": "459657c5-bdb7-4e43-c4b9-04cfe507fad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted word for 'jcdhx': almog\n"
          ]
        }
      ],
      "source": [
        "# Function to preprocess the input text\n",
        "def preprocess_input(text):\n",
        "  no_space_word = \"\"\n",
        "  for i in range(len(text)):\n",
        "    if(text[i] != ' '):\n",
        "      no_space_word += text[i]\n",
        "  encoded_word = [char_to_int(char) for char in no_space_word]\n",
        "  padded_sequence = pad_sequences([encoded_word], maxlen=max_word_length, padding='post')\n",
        "  return padded_sequence\n",
        "\n",
        "# Prepare the input text\n",
        "# machine = yolopws\n",
        "input_text = \"jcdhx\"\n",
        "preprocessed_input = preprocess_input(input_text)\n",
        "\n",
        "# Make prediction using the model\n",
        "predicted_sequence = model.predict(preprocessed_input)\n",
        "\n",
        "# Decode the predicted sequence (optional)\n",
        "def decode_sequence(sequence):\n",
        "  decoded_word = [int_to_char(i.argmax()) for i in sequence.squeeze(0)]\n",
        "  # print(\"decode_sequence: \", decoded_word)\n",
        "  return ''.join(decoded_word)  # Remove brackets and trailing spaces\n",
        "\n",
        "# Print the predicted sequence\n",
        "predicted_word = decode_sequence(predicted_sequence)\n",
        "print(f\"Predicted word for '{input_text}': {predicted_word[:len(input_text)]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K0l9JlVCHxMC",
        "rELTzlm0KsHH",
        "uSTa-drqIM7F",
        "zj8XY06fLiQw",
        "vaSV806rLtGU",
        "qgWJZz-MDAXM",
        "oqumqnWfaxoc",
        "T7udnB0xcWMl",
        "NUEsR5nmcgJY",
        "ZDk31U76cjNV",
        "czd47ANosnzm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}